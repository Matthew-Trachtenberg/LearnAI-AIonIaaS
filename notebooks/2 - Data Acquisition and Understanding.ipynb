{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/solutions-microsoft-logo-small.png)\n",
    "<img src=\"assets/ai.jpg\" style=\"height:200px;float:right;vertical-align:text-top\">\n",
    "\n",
    "## Artificial Intelligence on IaaS++\n",
    "\n",
    "This is part 3 of a 7-part workshop. The Jupyter Notbooks we are using are arranged in the same order as the Team Data Science Process: \n",
    "\n",
    "0 - [Introduction and Setup](./0%20-%20Introduction.ipynb)\n",
    "\n",
    "1 - [Business Understanding](./1%20-%20Business%20Understanding.ipynb)\n",
    "\n",
    "2 - *(This Module)* [Data Acquisition and Understanding](./2%20-%20Data%20Acquisition%20and%20Understanding.ipynb)\n",
    "\n",
    "3 - [Modeling](./3%20-%20Modeling.ipynb)\n",
    "\n",
    "4 - [Deployment](./4%20-%20Deployment.ipynb)\n",
    "\n",
    "5 - [Customer Acceptance](./5%20-%20Customer%20Acceptance.ipynb)\n",
    "\n",
    "6 - [Workshop Wrap-up](./6%20-%20Workshop%20Wrap-up.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "<h3><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/check.png\">Phase Two - Data Acquisition and Understanding</h3>\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-data)\n",
    "\n",
    "The Data Aquisition and Understanding phase of the TDSP you ingest or access data from various locations to answer the questions the organization has asked. In most cases, this data will be in multiple locations. Once the data is ingested into the system, you’ll need to examine it to see what it holds. All data needs cleaning, so after the inspection phase, you’ll replace missing values, add and change columns. You’ll cover more extensive Data Wrangling tasks in other labs.\n",
    "\n",
    "In this section, we’ll use a file-based dataset source to train our model, using three sources of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goals**\n",
    "\n",
    "  - Produce a clean, high-quality data set whose relationship to the target variables is understood. Locate the data set in the appropriate analytics environment so you are ready to model.\n",
    "  - Develop a solution architecture of the data pipeline that refreshes and scores the data regularly.\n",
    "\n",
    "**How to do it**\n",
    "\n",
    "  - Ingest the data into the target analytic environment.\n",
    "  - Explore the data to determine if the data quality is adequate to answer the question.\n",
    "  - Set up a data pipeline to score new or regularly refreshed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><img style=\"float: right; margin: 0px 15px 15px 0px;\" src=\"./assets/aml-logo.png\"><b>More information on Using Azure Machine Learning for this Phase:</b></p>\n",
    "\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Load data into storage environments for analytics](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/ingest-data)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Explore data in the Team Data Science Process](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/explore-data)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Sample data in Azure blob containers, SQL Server, and Hive tables](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/sample-data)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Access datasets with Python using the Azure Machine Learning Python client library](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/python-data-access)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data description\n",
    "\n",
    "<img src=\"assets/files.jpg\" style=\"height:25px;float:right;vertical-align:text-top\">\n",
    "\n",
    "This workshop uses three data sets as inputs in the files **PM_train.txt**, **PM_test.txt**, and **PM_truth.txt** that we will download in code and persist locally:\n",
    "\n",
    "*Train data:* This is the aircraft engine run-to-failure data. The train data (*PM_train.txt*) consists of multiple, multivariate time series with cycle as the time unit. It includes 21 sensor readings for each cycle.\n",
    "  Each time series is generated from a different engine of the same type. Each engine starts with different degrees of initial wear and some unique manufacturing variation. This information is unknown to the user.\n",
    "  In this simulated data, the engine is assumed to be operating normally at the start of each time series. It starts to degrade at some point during the series of the operating cycles. The degradation progresses and grows in magnitude.\n",
    "  When a predefined threshold is reached, the engine is considered unsafe for further operation. The last cycle of each time series is the failure point of that engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Test data:* The aircraft engine operating data, without failure events recorded. The test data (*PM_test.txt*) has the same data schema as the training data. The only difference is that the data does not indicate when the failure occurs (the last time period does not represent the failure point). It is not known how many more cycles this engine can last before it fails.\n",
    "\n",
    "*Truth data:* The information of true remaining cycles for each engine in the testing data. The ground truth data provides the number of remaining working cycles for the engines in the testing data.\n",
    "\n",
    "This data is from simulated aircraft values, from 21 sensors to predict when an aircraft engine will fail in the future so that maintenance can be planned in advance. The data ingestion notebook will download the simulated predicitive maintenance data sets from a public Azure Blob Storage. Labels are created from the `truth` data and joined to the `training` and `test` data. After some preliminary data cleaning and verification, the results are stored in a local (to the notebook server) folder for analysis.\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab 2.0 - Ingest data from a local source\n",
    "\n",
    "<img src=\"assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "We will be reusing the raw simulated data files from the [Predictive Maintenance](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) tutorial. The notebook programatically downloads these files from http://azuremlsamples.azureml.net/templatedata/. \n",
    "\n",
    "The three data files are:\n",
    "\n",
    "  - `PM_train.txt`\n",
    "  - `PM_test.txt`\n",
    "  - `PM_truth.txt`\n",
    "    \n",
    "This notebook labels the train and test set and does some preliminary cleanup. We'll also create some summary graphics for each data set to verify the data download, and store the resulting data sets in a local folder.\n",
    "\n",
    "Instructions:\n",
    " 1. Run the Python Code in the cells below. \n",
    " \n",
    " #### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">Ensure that you have the three datasets loaded into the variables successfully, and that you see the graphical output of the basic analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Project and Data Ingestion setup:\n",
    "# Import libraries you'll need thorughout the labs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Libraries to display graphics of your exploration\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import urllib\n",
    "\n",
    "# Logging could optionally be placed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The raw train data is stored at this location:\n",
    "basedataurl = \"http://azuremlsamples.azureml.net/templatedata/\"\n",
    "# <TODO: Find or make the real data source>\n",
    "\n",
    "# We will store each of these data sets in a local persistance folder. Change this \n",
    "# location for production, or use a call out to Azure or other online storage.\n",
    "SHARE_ROOT = '/gpuclass/data/'\n",
    "\n",
    "# These file names detail where we store each data file. \n",
    "TRAIN_DATA = 'PM_train_files.pkl'\n",
    "TEST_DATA = 'PM_test_files.pkl'\n",
    "TRUTH_DATA = 'PM_truth_files.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data Ingestion\n",
    "In this section, we ingest the training, test and ground truth datasets from a remote location. \n",
    "\n",
    "The training data consists of multiple multivariate time series with `cycle` as the time unit, together with 21 sensor readings and 3 settings for each cycle. Each time series can be assumed as being generated from a different engine of the same type. The testing data has the same data schema as the training data, except that the data does not indicate *when* the failure occurs. The ground truth data provides the number of remaining working cycles for the engines in the testing data. (You can find more details about the type of data used for this notebook at [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3).)\n",
    "\n",
    "The training data consists of data from 100 engines (`id`) in the form of multivariate time series with `cycle` as the unit of time with 21 sensor readings `s1:s21` and 3 operational `setting` features for each `cycle`. In this simulated data, an engine is assumed to be operating normally at the start of each time series. Engine degradation progresses and grows in magnitude until a predefined threshold is reached where the engine is considered unsafe for further operation. In this simulation, the last cycle in each time series can be considered as the failure point of the corresponding engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load raw training data from Azure blob\n",
    "train_df = 'PM_train.txt'\n",
    "\n",
    "# Download the file once, and only once.\n",
    "if not os.path.isfile(train_df):\n",
    "    urllib.request.urlretrieve(basedataurl+train_df, train_df)\n",
    "\n",
    "# read training data \n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                    's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                    's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "# Display the results\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The testing data has the same data schema as the training data except that the failure point is unknown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load raw data from Azure blob\n",
    "test_df = 'PM_test.txt'\n",
    "\n",
    "# Download the file once, and only once.\n",
    "if not os.path.isfile(test_df):\n",
    "    urllib.request.urlretrieve(basedataurl+test_df, test_df)\n",
    "    \n",
    "# read test data\n",
    "test_df = pd.read_csv('PM_test.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = train_df.columns\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ground truth data provides the number of remaining working cycles (Ramaining useful life (RUL)) for the engines in the testing data. We use this data to evaluation the model after training with the training data set only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load raw data from Azure blob\n",
    "truth_df = 'PM_truth.txt'\n",
    "\n",
    "# Download the file once, and only once.\n",
    "if not os.path.isfile(truth_df):\n",
    "    urllib.request.urlretrieve(basedataurl+truth_df, truth_df)\n",
    "    \n",
    "# read ground truth data\n",
    "truth_df = pd.read_csv('PM_truth.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "truth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data Preprocessing\n",
    "We next generate labels for the training data. Since the last observation is assumed to be a failure point, we can calculate the Remaining Useful Life (`RUL`) for every cycle in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Data Labeling - generate column RUL\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using RUL, we can create a label indicating time to failure. We define a boolean (`True\\False`) value for `label1` indicating the engine will fail within 30 days (RUL $<= 30$). We can also define a multiclass `label2` $\\in \\{0, 1, 2\\}$ indicating {Healthy, RUL <=30, RUL <=15} cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generate label columns for training data\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "\n",
    "# Label1 indicates a failure will occur within the next 30 cycles.\n",
    "# 1 indicates failure, 0 indicates healthy \n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "\n",
    "# label2 is multiclass, value 1 is identical to label1,\n",
    "# value 2 indicates failure within 15 cycles\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) , the `cycle` column is also used for training, so we will also include it. Here, we normalize the columns in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MinMax normalization\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we prepare the test data. We normalize the data using the same parameters from the training data normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we use the ground truth dataset to generate labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generate column max for test data\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then create the same labels as used for the `training` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generate label columns w0 and w1 for test data\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "### Lab 2.1 - Data Exploration and Understanding\n",
    "\n",
    "<img src=\"assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "One critical advantage of LSTMs is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering as computing rolling averages over large window sizes (i.e. 50 cycles) may lead to loss of information due to smoothing and abstracting of values over such a long period. While feature engineering over large window sizes may not make sense, LSTMs are able to use all the information in the window as input. We first look at an example of the sensor values for 50 cycles prior to the failure for engine `id = 3`. \n",
    "\n",
    "Instructions:\n",
    " 1. Run the cells below one at a time.\n",
    "\n",
    "#### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">Ensure that you understand the data, it's layout, and know any missing values in the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preparing data for visualizations \n",
    "# window of 50 cycles prior to a failure point for engine id 3\n",
    "engine_id3 = test_df[test_df['id'] == 3]\n",
    "engine_id3_50cycleWindow = engine_id3[engine_id3['RUL'] <= engine_id3['RUL'].min() + 50]\n",
    "cols1 = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10']\n",
    "engine_id3_50cycleWindow1 = engine_id3_50cycleWindow[cols1]\n",
    "cols2 = ['s11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "engine_id3_50cycleWindow2 = engine_id3_50cycleWindow[cols2]\n",
    "\n",
    "# plotting sensor data for engine ID 3 prior to a failure point - sensors 1-10 \n",
    "ax1 = engine_id3_50cycleWindow1.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plotting sensor data for engine ID 3 prior to a failure point - sensors 11-21 \n",
    "ax2 = engine_id3_50cycleWindow2.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Persist the data sets\n",
    "\n",
    "With the training and testing data created, we can turn our attention to modelling the engine failures. In order to pass the data set to out next notebook, we will write the data to a folder shared within the Azure ML project. https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files\n",
    "\n",
    "The `Code\\2_model_building_and_evaluation.ipynb` the code will then read these data files and train a LSTM network to predict the probability of engine failure within the next 30 cycles using the previous 50 cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert \n",
    "# store it for later manipulations in subsequent notebooks.\n",
    "train_df.to_pickle(SHARE_ROOT + TRAIN_DATA)\n",
    "test_df.to_pickle(SHARE_ROOT + TEST_DATA)\n",
    "\n",
    "print(\"Data stream saved at: \" + SHARE_ROOT + TEST_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "### Phase 2 wrap-up\n",
    "\n",
    "<img src=\"assets/wrapup.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "<p>This module covered the Data Acquisition and Understanding phase of the solution.</p>\n",
    "\n",
    "<p>The Notebooks are arranged in the same order as the Team Data Science Process:</p> \n",
    "\n",
    "0 - [Introduction and Setup](./0%20-%20Introduction.ipynb)\n",
    "\n",
    "1 - [Business Understanding](./1%20-%20Business%20Understanding.ipynb)\n",
    "\n",
    "2 - *(This module)* [Data Acquisition and Understanding](./2%20-%20Data%20Acquisition%20and%20Understanding.ipynb)\n",
    "\n",
    "3 - *(Proceed to this Notebook Next)* [Modeling](./3%20-%20Modeling.ipynb)\n",
    "\n",
    "4 - [Deployment](./4%20-%20Deployment.ipynb)\n",
    "\n",
    "5 - [Customer Acceptance](./5%20-%20Customer%20Acceptance.ipynb)\n",
    "\n",
    "6 - [Workshop Wrap-up](./6%20-%20Workshop%20Wrap-up.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
