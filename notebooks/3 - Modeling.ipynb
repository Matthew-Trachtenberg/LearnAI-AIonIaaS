{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*(Delete this cell before release)*\n",
    "\n",
    "STATUS: In-Work\n",
    "\n",
    "REFERENCES:\n",
    "    \n",
    "TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/solutions-microsoft-logo-small.png)\n",
    "<img src=\"assets/ai.jpg\" style=\"height:200px;float:right;vertical-align:text-top\">\n",
    "\n",
    "## Artificial Intelligence on IaaS++\n",
    "\n",
    "This is part 4 of a 7-part workshop. The Jupyter Notbooks we are using are arranged in the same order as the Team Data Science Process: \n",
    "\n",
    "0 - [Introduction and Setup](./0%20-%20Introduction.ipynb)\n",
    "\n",
    "1 - [Business Understanding](./1%20-%20Business%20Understanding.ipynb)\n",
    "\n",
    "2 - [Data Acquisition and Understanding](./2%20-%20Data%20Acquisition%20and%20Understanding.ipynb)\n",
    "\n",
    "3 - *(This Module)* [Modeling](./3%20-%20Modeling.ipynb)\n",
    "\n",
    "4 - [Deployment](./4%20-%20Deployment.ipynb)\n",
    "\n",
    "5 - [Customer Acceptance](./5%20-%20Customer%20Acceptance.ipynb)\n",
    "\n",
    "6 - [Workshop Wrap-up](./6%20-%20Workshop%20Wrap-up.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "REFERFENCES: Scale-out- https://docs.microsoft.com/en-us/azure/machine-learning/preview/scenario-aerial-image-classification \n",
    "\n",
    "<h1><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/check.png\">Phase Three - Modeling</h1>\n",
    "\n",
    "Read the [Documentation Reference here](TODO)\n",
    "\n",
    "**Goals**\n",
    "  - Determine the optimal data features for the machine-learning model.\n",
    "  - Create an informative machine-learning model that predicts the target most accurately.\n",
    "  - Create a machine-learning model that's suitable for production.\n",
    "\n",
    "**How to do it**\n",
    "  - Feature engineering: Create data features from the raw data to facilitate model training.\n",
    "  - Model training: Find the model that answers the question most accurately by comparing their success metrics.\n",
    "  - Determine if your model is suitable for production.\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><img style=\"float: right; margin: 0px 15px 15px 0px;\" src=\"./assets/aml-logo.png\"><b>Using Azure Machine Learning for this Phase:</b></p>\n",
    "\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Feature engineering in data science](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/create-features)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Feature selection](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/select-features)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Choose an algorithms for Microsoft Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice?toc=%2Fen-us%2Fazure%2Fmachine-learning%2Fteam-data-science-process%2Ftoc.json&bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "### Lab 3.0 - Model Building & Evaluation\n",
    "\n",
    "<img src=\"assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "Using the training and test data sets we constructed in the previous Jupyter notebook, this notebook builds a LSTM network for scenerio described at [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) to predict failure in aircraft engines. We will store the model for deployment in an Azure web service which we build in the next Jupyter notebook. We'll start with setting up the environment for this phase:\n",
    "\n",
    "Instructions:\n",
    " 1. Run the cells below one at a time.\n",
    "\n",
    "#### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">Ensure that each cell runs, and produces the correct prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the libraries for this phase:\n",
    "\n",
    "import keras\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from sklearn import datasets\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "run_logger = get_azureml_logger()\n",
    "run_logger.log('amlrealworld.predictivemaintenanceforpm.modelbuildingandevaluation','true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load the feature data set\n",
    "\n",
    "We have previously created the labeled data set in the previous phase and stored it in local persistant storage. We define the storage locations for both the notebook input and output here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will store each of these data sets in a local persistance folder\n",
    "SHARE_ROOT = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "\n",
    "# These file names detail the data files. \n",
    "TRAIN_DATA = 'PM_train_files.pkl'\n",
    "TEST_DATA = 'PM_test_files.pkl'\n",
    "\n",
    "# We'll serialize the model in json format\n",
    "LSTM_MODEL = 'modellstm.json'\n",
    "\n",
    "# and store the weights in h5\n",
    "MODEL_WEIGHTS = 'modellstm.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load the training and testing data and dump a short summary of the resulting DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(SHARE_ROOT + TRAIN_DATA)\n",
    "train_df.head(10)\n",
    "\n",
    "test_df = pd.read_pickle(SHARE_ROOT + TEST_DATA)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling\n",
    "\n",
    "The traditional predictive maintenance machine learning models are based on feature engineering, the manual construction of variable using domain expertise and intuition. This usually makes these models hard to reuse as the feature are specific to the problem scenario and the available data may vary between customers. Perhaps the most attractive advantage of deep learning they automatically do feature engineering from the data, eliminating the need for the manual feature engineering step. For this workshop, we'll use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When using LSTM's in a time-series domain, one important parameter is the sequence length - the window to examine for the failure signal. This may be viewed as picking a `window_size` (i.e. 5 cycles) for calculating the rolling features in the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3). The rolling features included rolling mean and rolling standard deviation over the 5 cycles for each of the 21 sensor values. In deep learning, we allow the LSTM's to extract abstract features out of the sequence of sensor values within the window. The expectation is that patterns within these sensor values will be automatically encoded by the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another critical advantage of LSTM's is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering. Computing rolling averages over a window size of 50 cycles may lead to loss of information due to smoothing over such a long period. LSTMs are able to use larger window sizes and use all the information in the window as input. This workshop illustrates the LSTM approach to binary classification using a sequence_length of 50 cycles to predict the probability of engine failure within 30 days:\n",
    "(http://colah.github.io/posts/2015-08-Understanding-LSTMs/ contains more information on the details of LSTM networks.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We use the [Keras LSTM](https://keras.io/layers/recurrent/) with [Tensorflow](https://tensorflow.org) as a backend. Here layers expect an input in the shape of an array of 3 dimensions (samples, time steps, features) where samples is the number of training sequences, time steps is the look back window or sequence length and features is the number of features of each sequence at each time step.\n",
    "\n",
    "We define a function to generate this array, as we'll use it repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The sequences are built from the features (sensor and settings) values across the time steps (cycles) within each engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "key_cols = ['id', 'cycle']\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "input_features = test_df.columns.values.tolist()\n",
    "sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "sensor_cols = [x for x in sensor_cols if x not in set(label_cols)]\n",
    "sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "# The time is sequenced along\n",
    "# This may be a silly way to get these column names, but it's relatively clear\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "print(sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also create a function to label these sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will only be using the LSTM to predict failure within the next 30 days (`label1`). To predict other labels, we could change this call before building the LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM Network\n",
    "\n",
    "Building a Neural Net requires determining the network architecture. In this scenario we will build a network of only 2 layers, with a dropout. The first LSTM layer with 100 units, one for each input sequence, followed by another LSTM layer with 50 units. We will also apply dropout each LSTM layer to control overfitting. The final dense output layer employs a sigmoid activation corresponding to the binary classification requirement. We'll set that up next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# build the network\n",
    "# Feature weights\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# The first layer\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "\n",
    "# Plus a 20% dropout rate\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# The second layer\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "\n",
    "# Plus a 20% dropout rate\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Dense sigmoid layer\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "\n",
    "# With adam optimizer and a binary crossentropy loss. We will opimize for model accuracy.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Verify the architecture \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It takes about 15 seconds per epoch to build this model on a DS4_V2 standard [Data Science Virtual Machine for Linux (Ubuntu)](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.linux-data-science-vm-ubuntu) using only CPU compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit the network\n",
    "model.fit(seq_array, # Training features\n",
    "          label_array, # Training labels\n",
    "          epochs=10,   # We'll stop after 10 epochs\n",
    "          batch_size=200, # \n",
    "          validation_split=0.10, # Use 10% of data to evaluate the loss. (val_loss)\n",
    "          verbose=1, #\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', # Monitor the validation loss\n",
    "                                                     min_delta=0,    # until it doesn't change (or gets worse)\n",
    "                                                     patience=5,  # patience > 1 so it continutes if it is not consistently improving\n",
    "                                                     verbose=0, \n",
    "                                                     mode='auto')]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We optimized the network weights on the training set accuracy, which we examine here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Training Accurracy: {}'.format(scores[1]))\n",
    "run_logger.log(\"Training Accuracy\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can examine the training set performance by looking at the model confusion matrix. Accurate predictions lie along the diagonal of the matrix, errors are on the off diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "print('Training Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we have many more healthy cycles than failure cycles, we also look at precision and recall. In all cases, we assume the model threshold is at `$Pr = 0.5$`. In order to tune this, we need to look at a test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compute precision and recall\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print( 'Training Precision: ', precision, '\\n', 'Training Recall: ', recall, '\\n', 'Training F1 Score:', f1)\n",
    "run_logger.log(\"Training Precision\", precision)\n",
    "run_logger.log(\"Training Recall\", recall)\n",
    "run_logger.log(\"Training F1 Score\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model testing\n",
    "Next, we look at the performance on the test data. Only the last cycle data for each engine id in the test data is kept for testing purposes. In order to compare the results to the template, we pick the last sequence for each id in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "seq_array_test_last.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also ned the test set labels in the correct format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "\n",
    "label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "label_array_test_last.shape\n",
    "\n",
    "print(seq_array_test_last.shape)\n",
    "print(label_array_test_last.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can check the model with the test data. We report the model accuracy on the test set, and compare it to the training accuracy. By definition, the training accuracy should be optimistic since the model was optimized for those observations. The test set accuracy is more general, and simulates how the model was intended to be used to predict forward in time. This is the number we should use for reporting how the model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# test metrics\n",
    "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "print('Test Accurracy: {}'.format(scores_test[1]))\n",
    "run_logger.log(\"Test Accuracy\", scores_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...And the same for the test set confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "y_pred_test = model.predict_classes(seq_array_test_last)\n",
    "y_true_test = label_array_test_last\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The confusion matrix uses absolute counts, so comparing the test and training set confusion matrices is difficult. Instead, it is  better to use `precision` and `recall`. \n",
    "\n",
    " * _Precision_ measures how accurate the model predicts failures. What percentage of the failure predictions are actually failures.\n",
    " * _Recall_ measures how well the model captures those failures. What percentage of the true failures did this model capture?\n",
    " \n",
    "These measures are tightly coupled, and you can typically only choose to maximize one of them (by manipulating the probability threshold) and have to accept the other as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compute precision and recall\n",
    "precision_test = precision_score(y_true_test, y_pred_test)\n",
    "recall_test = recall_score(y_true_test, y_pred_test)\n",
    "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "print( 'Test Precision: ', precision_test, '\\n', 'Test Recall: ', recall_test, '\\n', 'Test F1 Score:', f1_test)\n",
    "run_logger.log(\"Test Precision\", precision_test)\n",
    "run_logger.log(\"Test Recall\", recall_test)\n",
    "run_logger.log(\"Test F1 Score\", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saving the model  \n",
    "\n",
    "The LSTM network is made up of two components, the architecture and the model weights. We'll save these model components in two files, the architecture in a `json` file that the `keras` package can use to rebuild the model, and the weights in an `HDF5` heirachy that rebuild the exact model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model for operationalization: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "import os\n",
    "import h5py\n",
    "from sklearn import datasets \n",
    " \n",
    "# save model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(LSTM_MODEL, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(MODEL_WEIGHTS)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To test the `save` operations, we can reload the model files into a test model `loaded_model` and rescore the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "print(keras.__version__)\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(LSTM_MODEL, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(MODEL_WEIGHTS)\n",
    "\n",
    "loaded_model.compile('sgd','mse')\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now the model constructed from storage can be used to predict the probability of engine failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score = loaded_model.predict_proba(seq_array,verbose=1)\n",
    "print(score.shape)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Persist the model\n",
    "\n",
    "In order to pass the model to the next notebook, we will write the model files to the shared folder within the Azure ML  project. https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files\n",
    "\n",
    "In the next phase, we will create the functions needed to operationalize and deploy any model to get realtime predictions. The artifacts created will be stored in one of your Azure storage containers for you to deploy and test your own web service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with open(SHARE_ROOT + LSTM_MODEL, 'wt') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    print(\"json file written shared folder\")\n",
    "    json_file.close()\n",
    "    \n",
    "model.save_weights(os.path.join(SHARE_ROOT, MODEL_WEIGHTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "### LabX.0 - (Optional) - Train Model using Microsoft Azure Batch AI\n",
    "\n",
    "<img src=\"assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "TODO:\n",
    "\n",
    "Instructions:\n",
    " 1. TODO:\n",
    "\n",
    "#### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">TODO:</p>\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/batch-ai/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "### Phase 3 wrap-up\n",
    "\n",
    "<img src=\"assets/wrapup.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "<p>This module covered the Modeling phase of the solution.</p>\n",
    "\n",
    "<p>The Notebooks are arranged in the same order as the Team Data Science Process:</p> \n",
    "\n",
    "0 - [Introduction and Setup](./0%20-%20Introduction.ipynb)\n",
    "\n",
    "1 - [Business Understanding](./1%20-%20Business%20Understanding.ipynb)\n",
    "\n",
    "2 - [Data Acquisition and Understanding](./2%20-%20Data%20Acquisition%20and%20Understanding.ipynb)\n",
    "\n",
    "3 - *(This module)* [Modeling](./3%20-%20Modeling.ipynb)\n",
    "\n",
    "4 - *(Proceed to this Notebook Next)* [Deployment](./4%20-%20Deployment.ipynb)\n",
    "\n",
    "5 - [Customer Acceptance](./5%20-%20Customer%20Acceptance.ipynb)\n",
    "\n",
    "6 - [Workshop Wrap-up](./6%20-%20Workshop%20Wrap-up.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
