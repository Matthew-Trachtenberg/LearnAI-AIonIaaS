{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"./assets/solutions-microsoft-logo-small.png\">\n",
    "\n",
    "# AI on IaaS++\n",
    "\n",
    "## Microsoft Cloud and AI Team\n",
    "\n",
    "The Team Data Science Process (TDSP) is an agile, iterative data science methodology to deliver predictive analytics solutions and intelligent applications efficiently. TDSP helps improve team collaboration and learning. It contains a distillation of the best practices and structures from Microsoft and others in the industry that facilitate the successful implementation of data science initiatives. The goal is to help companies fully realize the benefits of their analytics program.\n",
    "\n",
    "TDSP comprises of the following key components:\n",
    "\n",
    " - A data science lifecycle definition\n",
    " - A standardized project structure\n",
    "    Infrastructure and resources for data science projects\n",
    "    Tools and utilities for project execution\n",
    "    \n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/aml-logo.png\">**Note:** \n",
    "    \n",
    "*You can follow a complete example of this process using Azure Machine Learning* \n",
    "</br>\n",
    "\n",
    "- [\"Biomedical entity recognition using Team Data Science Process (TDSP) Template\"](https://docs.microsoft.com/en-us/azure/machine-learning/preview/scenario-tdsp-biomedical-recognition?toc=%2Fen-us%2Fazure%2Fmachine-learning%2Fteam-data-science-process%2Ftoc.json&bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json)</p>\n",
    "\n",
    "*This workshop guides you through a series of exercises you can use to learn to implement the TDSP in your Data Science project, using only Python in a Notebook. You can change the **Setup** and **Lab** cells in this Notebook to use another language, another platform, and with more or fewer prompts based on your audience's needs.*\n",
    "\n",
    "For the labs below, Look for the sections marked: \n",
    "\n",
    "`# <TODO: REPLACE THIS COMMENT WITH PYTHON CODE>`\n",
    "\n",
    "There may be one line needed, but most often more than that - read the entire code snippet to see what you need to do. \n",
    "\n",
    "[Try to figure out the labs yourself, then search the web, then ask your neighbor - and if you're really stuck, check the answer-sheet](.\\AnswerKey.txt) \n",
    "\n",
    "TODO: \n",
    "3.\tDeploying models: \n",
    "a.\tAzureML Container/Kubernetes  based deployment (Realtime now, Batch coming soon) – Maybe a pointer to existing AzureML material \n",
    "b.\tDeploying DL/Spark models for batch scoring on aztk/BatchAI/Databricks\n",
    "c.\tDeploying models on Tensorflow Serving\n",
    "d.\tOther ways to deploy models – Natively on DSVM (for small data) using Flask etc, Azure Functions, App Services\n",
    "e.\tFuture ONNX Model export and deployment (TBD)\n",
    " \n",
    "    \n",
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "\n",
    "REFRENCES: IOT - https://github.com/Azure/ai-toolkit-iot-edge \n",
    "<h1><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/check.png\">Phase Four - Deployment</h1>\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-deployment)\n",
    "\n",
    "**Goal**\n",
    " - Deploy models with a data pipeline to a production or production-like environment for final user acceptance\n",
    "\n",
    "**How to do it**\n",
    "  - Deploy the model and pipeline to a production or production-like environment for application consumption\n",
    "\n",
    "<p><img style=\"float: right; margin: 0px 15px 15px 0px;\" src=\"./assets/aml-logo.png\"><b>Using Azure Machine Learning for this Phase:</b></p>\n",
    "\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Deploy models in production](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/deploy-models-in-production)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Configure your environment to operationalize](https://docs.microsoft.com/en-us/azure/machine-learning/preview/cli-for-azure-machine-learning#o16n)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Model management](https://docs.microsoft.com/en-us/azure/machine-learning/preview/deployment-setup-configuration)</p>\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "### Lab 4.0 - Operationalize\n",
    "Instructions:\n",
    "1. Deploy the model and pipeline to a production or production-like environment for application consumption to one or more targets:\n",
    "  - Online websites\n",
    "  - Spreadsheets\n",
    "  - Dashboards\n",
    "  - Line-of-business applications\n",
    "  - Back-end applications\n",
    "\n",
    "#### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">Save on disk a serialized model in Python (Pickle File) and create code that shows the artifacts (schema definition) needed to call it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export the model to outputs/model.pkl\n"
     ]
    }
   ],
   "source": [
    "#LAB4.0a - Create the Model File\n",
    "# serialize the best performing model on disk:\n",
    "print (\"Serialize the model to a model.pkl file in the root\")\n",
    "# <TODO: REPLACE THIS COMMENT WITH PYTHON CODE>\n",
    "\n",
    "#/LAB4.0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON sent to the prediction Model: \n",
      "\n",
      "{\"callfailurerate\": 0, \"education\": \"Bachelor or equivalent\", \"usesinternetservice\": \"No\", \"gender\": \"Male\", \"unpaidbalance\": 19, \"occupation\": \"Technology Related Job\", \"year\": 2015, \"numberofcomplaints\": 0, \"avgcallduration\": 663, \"usesvoiceservice\": \"No\", \"annualincome\": 168147, \"totalminsusedinlastmonth\": 15, \"homeowner\": \"Yes\", \"age\": 12, \"maritalstatus\": \"Single\", \"month\": 1, \"calldroprate\": 0.06, \"percentagecalloutsidenetwork\": 0.82, \"penaltytoswitch\": 371, \"monthlybilledamount\": 71, \"churn\": 0, \"numdayscontractequipmentplanexpiring\": 96, \"totalcallduration\": 5971, \"callingnum\": 4251078442, \"state\": \"WA\", \"customerid\": 1, \"customersuspended\": \"Yes\", \"numberofmonthunpaid\": 7, \"noadditionallines\": \"\\\\N\"} \n",
      "\n",
      "For the JSON string sent from the client, The prediction is returned as more JSON (0 = No churn, 1 = Churn): \n",
      "\n",
      "\"0\"\n"
     ]
    }
   ],
   "source": [
    "#LAB4.0b - Operationalization: Scoring the calls to the model\n",
    "# Prepare the web service definition before deploying\n",
    "# Import for the pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# load the model file\n",
    "global model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Import for handling the JSON file\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Set up a sample \"call\" from a client:\n",
    "input_df = \"{\\\"callfailurerate\\\": 0, \\\"education\\\": \\\"Bachelor or equivalent\\\", \\\"usesinternetservice\\\": \\\"No\\\", \\\"gender\\\": \\\"Male\\\", \\\"unpaidbalance\\\": 19, \\\"occupation\\\": \\\"Technology Related Job\\\", \\\"year\\\": 2015, \\\"numberofcomplaints\\\": 0, \\\"avgcallduration\\\": 663, \\\"usesvoiceservice\\\": \\\"No\\\", \\\"annualincome\\\": 168147, \\\"totalminsusedinlastmonth\\\": 15, \\\"homeowner\\\": \\\"Yes\\\", \\\"age\\\": 12, \\\"maritalstatus\\\": \\\"Single\\\", \\\"month\\\": 1, \\\"calldroprate\\\": 0.06, \\\"percentagecalloutsidenetwork\\\": 0.82, \\\"penaltytoswitch\\\": 371, \\\"monthlybilledamount\\\": 71, \\\"churn\\\": 0, \\\"numdayscontractequipmentplanexpiring\\\": 96, \\\"totalcallduration\\\": 5971, \\\"callingnum\\\": 4251078442, \\\"state\\\": \\\"WA\\\", \\\"customerid\\\": 1, \\\"customersuspended\\\": \\\"Yes\\\", \\\"numberofmonthunpaid\\\": 7, \\\"noadditionallines\\\": \\\"\\\\\\\\N\\\"}\"\n",
    "\n",
    "# Cleanup \n",
    "input_df_encoded = json.loads(input_df)\n",
    "input_df_encoded = pd.DataFrame([input_df_encoded], columns=input_df_encoded.keys())\n",
    "input_df_encoded = input_df_encoded.drop('year', 1)\n",
    "input_df_encoded = input_df_encoded.drop('month', 1)\n",
    "input_df_encoded = input_df_encoded.drop('churn', 1)\n",
    "\n",
    "# Pre-process scoring data consistent with training data\n",
    "columns_to_encode = ['customersuspended', 'education', 'gender', 'homeowner', 'maritalstatus', 'noadditionallines', 'occupation', 'state', 'usesinternetservice', 'usesvoiceservice']\n",
    "dummies = pd.get_dummies(input_df_encoded[columns_to_encode])\n",
    "input_df_encoded = input_df_encoded.join(dummies)\n",
    "input_df_encoded = input_df_encoded.drop(columns_to_encode, axis=1)\n",
    "\n",
    "columns_encoded = ['age', 'annualincome', 'calldroprate', 'callfailurerate', 'callingnum',\n",
    "       'customerid', 'monthlybilledamount', 'numberofcomplaints',\n",
    "       'numberofmonthunpaid', 'numdayscontractequipmentplanexpiring',\n",
    "       'penaltytoswitch', 'totalminsusedinlastmonth', 'unpaidbalance',\n",
    "       'percentagecalloutsidenetwork', 'totalcallduration', 'avgcallduration',\n",
    "       'customersuspended_No', 'customersuspended_Yes',\n",
    "       'education_Bachelor or equivalent', 'education_High School or below',\n",
    "       'education_Master or equivalent', 'education_PhD or equivalent',\n",
    "       'gender_Female', 'gender_Male', 'homeowner_No', 'homeowner_Yes',\n",
    "       'maritalstatus_Married', 'maritalstatus_Single', 'noadditionallines_\\\\N',\n",
    "       'occupation_Non-technology Related Job', 'occupation_Others',\n",
    "       'occupation_Technology Related Job', 'state_AK', 'state_AL', 'state_AR',\n",
    "       'state_AZ', 'state_CA', 'state_CO', 'state_CT', 'state_DE', 'state_FL',\n",
    "       'state_GA', 'state_HI', 'state_IA', 'state_ID', 'state_IL', 'state_IN',\n",
    "       'state_KS', 'state_KY', 'state_LA', 'state_MA', 'state_MD', 'state_ME',\n",
    "       'state_MI', 'state_MN', 'state_MO', 'state_MS', 'state_MT', 'state_NC',\n",
    "       'state_ND', 'state_NE', 'state_NH', 'state_NJ', 'state_NM', 'state_NV',\n",
    "       'state_NY', 'state_OH', 'state_OK', 'state_OR', 'state_PA', 'state_RI',\n",
    "       'state_SC', 'state_SD', 'state_TN', 'state_TX', 'state_UT', 'state_VA',\n",
    "       'state_VT', 'state_WA', 'state_WI', 'state_WV', 'state_WY',\n",
    "       'usesinternetservice_No', 'usesinternetservice_Yes',\n",
    "       'usesvoiceservice_No', 'usesvoiceservice_Yes']\n",
    "\n",
    "# Now that they are encoded, some values will be \"empty\". Fill those with 0's:\n",
    "for column_encoded in columns_encoded:\n",
    "    if not column_encoded in input_df_encoded.columns:\n",
    "        input_df_encoded[column_encoded] = 0\n",
    "\n",
    "# Return final prediction\n",
    "pred = model.predict(input_df_encoded)\n",
    "\n",
    "# (In production you would replace Print() statement here with some sort of return to JSON)\n",
    "print('JSON sent to the prediction Model:', '\\n')\n",
    "print(input_df, '\\n')\n",
    "print('For the JSON string sent from the client, The prediction is returned as more JSON (0 = No churn, 1 = Churn):', '\\n')\n",
    "print(json.dumps(str(pred[0])))\n",
    "\n",
    "#/LAB4.0b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "<h1>Phase 4 wrap-up</h1>\n",
    "\n",
    "This workshop introduced the Team Data Science Process, and walked you through each step of implementing it. Regardless of plaform or technology, you can use this process to guide your projects in Advanced Analytics from start to finish. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
